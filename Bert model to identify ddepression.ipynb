{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4edd37a-e68b-4be4-a780-35cd76311082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4572cc5-cdcf-45ab-b628-7578b8a80e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "      <th>flattened text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>expanded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369</td>\n",
       "      <td>[['okay', 'awesome', 'thank', 'you'], ['are', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['okay', 'awesome', 'thank', 'you', 'are', 'yo...</td>\n",
       "      <td>['okay', 'awesome', 'thank', 'you', 'are', 'yo...</td>\n",
       "      <td>['okay', 'awesome', 'thank', 'you', 'are', 'yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423</td>\n",
       "      <td>[['okay'], ['and', 'please'], ['yes'], ['feeli...</td>\n",
       "      <td>0</td>\n",
       "      <td>['okay', 'and', 'please', 'yes', 'feeling', 'w...</td>\n",
       "      <td>['okay', 'and', 'please', 'yes', 'feeling', 'w...</td>\n",
       "      <td>['okay', 'and', 'please', 'yes', 'feeling', 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436</td>\n",
       "      <td>[['that', 'was', 'big'], ['yes'], [\"I'm\", 'doi...</td>\n",
       "      <td>0</td>\n",
       "      <td>['that', 'was', 'big', 'yes', \"I'm\", 'doing', ...</td>\n",
       "      <td>['that', 'was', 'big', 'yes', 'I', \"'m\", 'doin...</td>\n",
       "      <td>['that', 'was', 'big', 'yes', 'I', 'am', 'doin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>[['all', 'right'], ['okay'], ['oh', 'actually'...</td>\n",
       "      <td>0</td>\n",
       "      <td>['all', 'right', 'okay', 'oh', 'actually', 'be...</td>\n",
       "      <td>['all', 'right', 'okay', 'oh', 'actually', 'be...</td>\n",
       "      <td>['all', 'right', 'okay', 'oh', 'actually', 'be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447</td>\n",
       "      <td>[['yeah', \"that's\", 'perfectly', 'fine'], [\"I'...</td>\n",
       "      <td>0</td>\n",
       "      <td>['yeah', \"that's\", 'perfectly', 'fine', \"I'm\",...</td>\n",
       "      <td>['yeah', 'that', \"'s\", 'perfectly', 'fine', 'I...</td>\n",
       "      <td>['yeah', 'that', 'is', 'perfectly', 'fine', 'I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression  \\\n",
       "0   369  [['okay', 'awesome', 'thank', 'you'], ['are', ...           0   \n",
       "1   423  [['okay'], ['and', 'please'], ['yes'], ['feeli...           0   \n",
       "2   436  [['that', 'was', 'big'], ['yes'], [\"I'm\", 'doi...           0   \n",
       "3   318  [['all', 'right'], ['okay'], ['oh', 'actually'...           0   \n",
       "4   447  [['yeah', \"that's\", 'perfectly', 'fine'], [\"I'...           0   \n",
       "\n",
       "                                      flattened text  \\\n",
       "0  ['okay', 'awesome', 'thank', 'you', 'are', 'yo...   \n",
       "1  ['okay', 'and', 'please', 'yes', 'feeling', 'w...   \n",
       "2  ['that', 'was', 'big', 'yes', \"I'm\", 'doing', ...   \n",
       "3  ['all', 'right', 'okay', 'oh', 'actually', 'be...   \n",
       "4  ['yeah', \"that's\", 'perfectly', 'fine', \"I'm\",...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  ['okay', 'awesome', 'thank', 'you', 'are', 'yo...   \n",
       "1  ['okay', 'and', 'please', 'yes', 'feeling', 'w...   \n",
       "2  ['that', 'was', 'big', 'yes', 'I', \"'m\", 'doin...   \n",
       "3  ['all', 'right', 'okay', 'oh', 'actually', 'be...   \n",
       "4  ['yeah', 'that', \"'s\", 'perfectly', 'fine', 'I...   \n",
       "\n",
       "                                       expanded_text  \n",
       "0  ['okay', 'awesome', 'thank', 'you', 'are', 'yo...  \n",
       "1  ['okay', 'and', 'please', 'yes', 'feeling', 'w...  \n",
       "2  ['that', 'was', 'big', 'yes', 'I', 'am', 'doin...  \n",
       "3  ['all', 'right', 'okay', 'oh', 'actually', 'be...  \n",
       "4  ['yeah', 'that', 'is', 'perfectly', 'fine', 'I...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('NLP_input.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18df5e8-8737-4ddb-943c-2f3d40b3982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns = ['tokenized_text','expanded_text','flattened text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e230cb94-df6f-418b-b0ef-52c84cdb67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "columns = ['Text Data']\n",
    "\n",
    "for item in columns:\n",
    "    df[item] = df[item].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbffcb0-07b6-4bba-99f4-41e8d602926b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369</td>\n",
       "      <td>[[okay, awesome, thank, you], [are, you, okay,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423</td>\n",
       "      <td>[[okay], [and, please], [yes], [feeling, well]...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436</td>\n",
       "      <td>[[that, was, big], [yes], [I'm, doing, fine], ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>[[all, right], [okay], [oh, actually, before, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447</td>\n",
       "      <td>[[yeah, that's, perfectly, fine], [I'm, feelin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression\n",
       "0   369  [[okay, awesome, thank, you], [are, you, okay,...           0\n",
       "1   423  [[okay], [and, please], [yes], [feeling, well]...           0\n",
       "2   436  [[that, was, big], [yes], [I'm, doing, fine], ...           0\n",
       "3   318  [[all, right], [okay], [oh, actually, before, ...           0\n",
       "4   447  [[yeah, that's, perfectly, fine], [I'm, feelin...           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19189f24-b187-431f-85f0-1426d5fbad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_data(text_data):\n",
    "    # For each sublist, join the words in lowercase with spaces and add a period\n",
    "    return [' '.join(word.lower() for word in sublist) + '.' for sublist in text_data]\n",
    "\n",
    "# Apply the function to each entry in the 'Text Data' column\n",
    "df['Processed Text Data'] = df['Text Data'].apply(process_text_data)\n",
    "df['Processed Text Data'] = df['Processed Text Data'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4e264e-9326-4f4b-9fae-55c5bdec462f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "      <th>Processed Text Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369</td>\n",
       "      <td>[[okay, awesome, thank, you], [are, you, okay,...</td>\n",
       "      <td>0</td>\n",
       "      <td>okay awesome thank you. are you okay with this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423</td>\n",
       "      <td>[[okay], [and, please], [yes], [feeling, well]...</td>\n",
       "      <td>0</td>\n",
       "      <td>okay. and please. yes. feeling well. where are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436</td>\n",
       "      <td>[[that, was, big], [yes], [I'm, doing, fine], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>that was big. yes. i'm doing fine. mexico. whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>[[all, right], [okay], [oh, actually, before, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>all right. okay. oh actually before that one t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447</td>\n",
       "      <td>[[yeah, that's, perfectly, fine], [I'm, feelin...</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah that's perfectly fine. i'm feeling great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression  \\\n",
       "0   369  [[okay, awesome, thank, you], [are, you, okay,...           0   \n",
       "1   423  [[okay], [and, please], [yes], [feeling, well]...           0   \n",
       "2   436  [[that, was, big], [yes], [I'm, doing, fine], ...           0   \n",
       "3   318  [[all, right], [okay], [oh, actually, before, ...           0   \n",
       "4   447  [[yeah, that's, perfectly, fine], [I'm, feelin...           0   \n",
       "\n",
       "                                 Processed Text Data  \n",
       "0  okay awesome thank you. are you okay with this...  \n",
       "1  okay. and please. yes. feeling well. where are...  \n",
       "2  that was big. yes. i'm doing fine. mexico. whe...  \n",
       "3  all right. okay. oh actually before that one t...  \n",
       "4  yeah that's perfectly fine. i'm feeling great ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482a9a2-812d-4a2d-bf5d-52a060819f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 13:30:37.847348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-25 13:30:37.847408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-25 13:30:37.849235: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 13:30:37.858542: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-25 13:30:38.855499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from time import sleep\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7448c18-66a2-4748-8851-2412e66c0c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with seed: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 145s 11s/step - loss: 0.6506 - accuracy: 0.6552 - val_loss: 0.5916 - val_accuracy: 0.7273\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 111s 10s/step - loss: 0.6170 - accuracy: 0.7011 - val_loss: 0.5894 - val_accuracy: 0.7273\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.5947 - accuracy: 0.7011 - val_loss: 0.5859 - val_accuracy: 0.7273\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.5982 - accuracy: 0.7011 - val_loss: 0.5904 - val_accuracy: 0.7273\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.5635 - accuracy: 0.7011 - val_loss: 0.5944 - val_accuracy: 0.7273\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 111s 10s/step - loss: 0.5020 - accuracy: 0.8046 - val_loss: 0.6307 - val_accuracy: 0.6818\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 110s 10s/step - loss: 0.4568 - accuracy: 0.8391 - val_loss: 0.6340 - val_accuracy: 0.6818\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.3054 - accuracy: 0.9655 - val_loss: 0.6447 - val_accuracy: 0.7273\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.5859 - accuracy: 0.7273\n",
      "Validation Loss: 0.5858870148658752, Validation Accuracy: 0.7272727489471436\n",
      "3/3 [==============================] - 13s 3s/step\n",
      "Validation F1 Score for Fold 1: 0.6124401913875598\n",
      "Fold 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 140s 11s/step - loss: 0.6532 - accuracy: 0.6636 - val_loss: 0.6284 - val_accuracy: 0.6818\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 110s 10s/step - loss: 0.6068 - accuracy: 0.7045 - val_loss: 0.6226 - val_accuracy: 0.6818\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.5991 - accuracy: 0.7045 - val_loss: 0.6193 - val_accuracy: 0.6818\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 111s 10s/step - loss: 0.5954 - accuracy: 0.7045 - val_loss: 0.6308 - val_accuracy: 0.6818\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.5923 - accuracy: 0.7045 - val_loss: 0.6151 - val_accuracy: 0.6818\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.5690 - accuracy: 0.7045 - val_loss: 0.6099 - val_accuracy: 0.6818\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.5326 - accuracy: 0.7045 - val_loss: 0.6221 - val_accuracy: 0.6818\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 111s 10s/step - loss: 0.4166 - accuracy: 0.8636 - val_loss: 0.6790 - val_accuracy: 0.6364\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.3054 - accuracy: 0.9773 - val_loss: 0.7252 - val_accuracy: 0.6818\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 111s 10s/step - loss: 0.2253 - accuracy: 0.9773 - val_loss: 0.7869 - val_accuracy: 0.6818\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 112s 10s/step - loss: 0.1482 - accuracy: 1.0000 - val_loss: 0.8537 - val_accuracy: 0.5909\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.6099 - accuracy: 0.6818\n",
      "Validation Loss: 0.6098926663398743, Validation Accuracy: 0.6818181872367859\n",
      "3/3 [==============================] - 13s 3s/step\n",
      "Validation F1 Score for Fold 2: 0.5528255528255529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../sample_data/model/model1 were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../sample_data/model/model1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Some layers from the model checkpoint at ../sample_data/model/model2 were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../sample_data/model/model2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "seed_values = list(range(6,7))\n",
    "\n",
    "for seed in seed_values:\n",
    "    print(f\"Running with seed: {seed}\")\n",
    "    data = df\n",
    "    # Set the seed for Python random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the seed for TensorFlow\n",
    "    tf.random.set_seed(seed)\n",
    "    # Initialize the stratified k-fold cross-validator\n",
    "    kf = StratifiedKFold(n_splits=k,random_state=seed, shuffle=True) # ensure the shuffling is the reproducible\n",
    "    # Load the pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    # Define a loss function and metrics\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")]\n",
    "    # Define an optimizer\n",
    "    early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss\n",
    "    patience=5,          # Stop training after 3 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best validation performance\n",
    "    )\n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    # tf.keras.optimizers.Optimizer(\n",
    "    # name='adam',\n",
    "    # learning_rate=0.001,\n",
    "    # weight_decay=0,\n",
    "    # clipnorm=None,\n",
    "    # clipvalue=None,\n",
    "    # global_clipnorm=None,\n",
    "    # use_ema=False,\n",
    "    # ema_momentum=0.99,\n",
    "    # ema_overwrite_frequency=None,\n",
    "    # jit_compile=True,\n",
    "    # )\n",
    "\n",
    "    # Split the dataset into features and target arrays\n",
    "    texts = df['Processed Text Data'].values\n",
    "    labels = data['depression'].values\n",
    "    fold_results = []\n",
    "    fold_f1_scores = []\n",
    "    # i=1\n",
    "    # Iterate through the k folds\n",
    "\n",
    "    for fold, (train_val_index, test_index) in enumerate(kf.split(texts, labels)):\n",
    "      # Set the seed for each fold (to ensure the reproducibility of each fold, these are fold-specific seeds)\n",
    "      tf.random.set_seed(seed)\n",
    "      np.random.seed(seed)\n",
    "      random.seed(seed)\n",
    "      # Split train_val set to train and validation sets in 8:2 ratio\n",
    "      train_index, valid_index = train_test_split(train_val_index, test_size=0.2, stratify=labels[train_val_index], random_state=seed) #ensure the splitting is reproducible\n",
    "      print(f\"Fold {fold + 1}/{k}\")\n",
    "      # Create train, validation, and test datasets for this fold\n",
    "      train_texts, train_labels = texts[train_index], labels[train_index]\n",
    "      valid_texts, valid_labels = texts[valid_index], labels[valid_index]\n",
    "      test_texts, test_labels = texts[test_index], labels[test_index]\n",
    "      # Tokenize the input texts for the fold\n",
    "      train_inputs = tokenizer(list(train_texts), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "      valid_inputs = tokenizer(list(valid_texts), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "      test_inputs = tokenizer(list(test_texts), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "      # Define TensorFlow Datasets for training and validation\n",
    "      train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_inputs), train_labels))\n",
    "      valid_dataset = tf.data.Dataset.from_tensor_slices((dict(valid_inputs), valid_labels))\n",
    "      test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_inputs), test_labels))\n",
    "      # Compile the model for this fold\n",
    "      model.compile(optimizer=opt, loss=loss_fn, metrics=metrics)\n",
    "      # Train the model for this fold\n",
    "      batch_size = 8\n",
    "      num_epochs = 20\n",
    "      model.fit(train_dataset.shuffle(100).batch(batch_size), epochs=num_epochs, validation_data=valid_dataset.batch(batch_size),callbacks=[early_stopping])\n",
    "      # Evaluate the model on the validation data for this fold\n",
    "      loss, accuracy = model.evaluate(valid_dataset.batch(batch_size))\n",
    "      print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "      valid_pred_probs = model.predict(valid_dataset.batch(batch_size))\n",
    "      valid_pred_labels = np.argmax(valid_pred_probs.logits, axis=1)\n",
    "      # Compute F1 score for this fold\n",
    "      f1 = f1_score(valid_labels, valid_pred_labels, average='weighted')\n",
    "      print(f\"Validation F1 Score for Fold {fold + 1}: {f1}\")\n",
    "      # Store the F1 score for this fold\n",
    "      fold_f1_scores.append(f1)\n",
    "\n",
    "      model_save_path = f\"../sample_data/model/model{fold + 1}\"\n",
    "      model.save_pretrained(model_save_path)\n",
    "      # Create a DataFrame for test data\n",
    "      test_data_df = pd.DataFrame({'Case':test_index,'text': test_texts, 'label': test_labels})\n",
    "      # Save the test data to a CSV file\n",
    "      test_data_csv_path = f\"../sample_data/dataset/test{fold + 1}.csv\"\n",
    "      test_data_df.to_csv(test_data_csv_path, index=False)\n",
    "    sleep(2)\n",
    "\n",
    "    final_results = []\n",
    "    for fold in range(k):\n",
    "        # Load the model for the current fold\n",
    "        model_path = f\"../sample_data/model/model{fold + 1}\"\n",
    "        loaded_model = TFBertForSequenceClassification.from_pretrained(model_path)\n",
    "        # Load the test data for the current fold\n",
    "        data_path = f\"../sample_data/dataset/test{fold + 1}.csv\"\n",
    "        data = pd.read_csv(data_path)\n",
    "\n",
    "        # Prepare to store results\n",
    "        predictions = []\n",
    "        predicted_probabilities_list0 = []\n",
    "        predicted_probabilities_list1 = []\n",
    "        # predicted_probabilities_list2 = []\n",
    "        # predicted_probabilities_list3 = []\n",
    "        # Iterate over each row in the data\n",
    "        for text in data['text']:\n",
    "            test_inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "            prediction = loaded_model(test_inputs)\n",
    "            predicted_probabilities = tf.nn.softmax(prediction.logits, axis=1).numpy()\n",
    "            predicted_class = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "            predictions.append(predicted_class[0])\n",
    "            predicted_probabilities_list0.append(predicted_probabilities[0][0])\n",
    "            predicted_probabilities_list1.append(predicted_probabilities[0][1])\n",
    "            # predicted_probabilities_list2.append(predicted_probabilities[0][2])\n",
    "            # predicted_probabilities_list3.append(predicted_probabilities[0][3])\n",
    "        # Store results in the DataFrame\n",
    "        data['predicted_class'] = predictions\n",
    "        data['Fold_Model'] = fold + 1\n",
    "        data['prob_class_0'] = predicted_probabilities_list0\n",
    "        data['prob_class_1'] = predicted_probabilities_list1\n",
    "        # data['prob_class_2'] = predicted_probabilities_list2\n",
    "        # data['prob_class_3'] = predicted_probabilities_list3\n",
    "\n",
    "        # Append the results of this fold to the final results\n",
    "        final_results.append(data)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    final_result_data = pd.concat(final_results)\n",
    "    sleep(2)\n",
    "    final_result_data.to_csv(f\"../sample_data/dataset/result_depression_epoch20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe71e90-ee5e-473c-b243-f8dcd72b556a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
