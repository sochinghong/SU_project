{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69abe79d-b10c-477c-baa1-dd6cab382680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64bbd654-1e4a-477f-9596-dd1cb6a6ed60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "      <th>flattened text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>expanded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369</td>\n",
       "      <td>[['okay', 'awesome', 'thank', 'you'], ['are', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['okay', 'awesome', 'thank', 'you', 'are', 'yo...</td>\n",
       "      <td>['okay', 'awesome', 'thank', 'you', 'are', 'yo...</td>\n",
       "      <td>['okay', 'awesome', 'thank', 'you', 'are', 'yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423</td>\n",
       "      <td>[['okay'], ['and', 'please'], ['yes'], ['feeli...</td>\n",
       "      <td>0</td>\n",
       "      <td>['okay', 'and', 'please', 'yes', 'feeling', 'w...</td>\n",
       "      <td>['okay', 'and', 'please', 'yes', 'feeling', 'w...</td>\n",
       "      <td>['okay', 'and', 'please', 'yes', 'feeling', 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436</td>\n",
       "      <td>[['that', 'was', 'big'], ['yes'], [\"I'm\", 'doi...</td>\n",
       "      <td>0</td>\n",
       "      <td>['that', 'was', 'big', 'yes', \"I'm\", 'doing', ...</td>\n",
       "      <td>['that', 'was', 'big', 'yes', 'I', \"'m\", 'doin...</td>\n",
       "      <td>['that', 'was', 'big', 'yes', 'I', 'am', 'doin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>[['all', 'right'], ['okay'], ['oh', 'actually'...</td>\n",
       "      <td>0</td>\n",
       "      <td>['all', 'right', 'okay', 'oh', 'actually', 'be...</td>\n",
       "      <td>['all', 'right', 'okay', 'oh', 'actually', 'be...</td>\n",
       "      <td>['all', 'right', 'okay', 'oh', 'actually', 'be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447</td>\n",
       "      <td>[['yeah', \"that's\", 'perfectly', 'fine'], [\"I'...</td>\n",
       "      <td>0</td>\n",
       "      <td>['yeah', \"that's\", 'perfectly', 'fine', \"I'm\",...</td>\n",
       "      <td>['yeah', 'that', \"'s\", 'perfectly', 'fine', 'I...</td>\n",
       "      <td>['yeah', 'that', 'is', 'perfectly', 'fine', 'I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression  \\\n",
       "0   369  [['okay', 'awesome', 'thank', 'you'], ['are', ...           0   \n",
       "1   423  [['okay'], ['and', 'please'], ['yes'], ['feeli...           0   \n",
       "2   436  [['that', 'was', 'big'], ['yes'], [\"I'm\", 'doi...           0   \n",
       "3   318  [['all', 'right'], ['okay'], ['oh', 'actually'...           0   \n",
       "4   447  [['yeah', \"that's\", 'perfectly', 'fine'], [\"I'...           0   \n",
       "\n",
       "                                      flattened text  \\\n",
       "0  ['okay', 'awesome', 'thank', 'you', 'are', 'yo...   \n",
       "1  ['okay', 'and', 'please', 'yes', 'feeling', 'w...   \n",
       "2  ['that', 'was', 'big', 'yes', \"I'm\", 'doing', ...   \n",
       "3  ['all', 'right', 'okay', 'oh', 'actually', 'be...   \n",
       "4  ['yeah', \"that's\", 'perfectly', 'fine', \"I'm\",...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  ['okay', 'awesome', 'thank', 'you', 'are', 'yo...   \n",
       "1  ['okay', 'and', 'please', 'yes', 'feeling', 'w...   \n",
       "2  ['that', 'was', 'big', 'yes', 'I', \"'m\", 'doin...   \n",
       "3  ['all', 'right', 'okay', 'oh', 'actually', 'be...   \n",
       "4  ['yeah', 'that', \"'s\", 'perfectly', 'fine', 'I...   \n",
       "\n",
       "                                       expanded_text  \n",
       "0  ['okay', 'awesome', 'thank', 'you', 'are', 'yo...  \n",
       "1  ['okay', 'and', 'please', 'yes', 'feeling', 'w...  \n",
       "2  ['that', 'was', 'big', 'yes', 'I', 'am', 'doin...  \n",
       "3  ['all', 'right', 'okay', 'oh', 'actually', 'be...  \n",
       "4  ['yeah', 'that', 'is', 'perfectly', 'fine', 'I...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('NLP_input.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "737ebf4a-1236-43cd-afd5-98cb1be4a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns = ['tokenized_text','expanded_text','flattened text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e5dbba6-b1df-41ff-8347-1729da5ee543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "columns = ['Text Data']\n",
    "\n",
    "for item in columns:\n",
    "    df[item] = df[item].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a87f7786-9478-4f33-9641-9ca5dc9c42a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369</td>\n",
       "      <td>[[okay, awesome, thank, you], [are, you, okay,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423</td>\n",
       "      <td>[[okay], [and, please], [yes], [feeling, well]...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436</td>\n",
       "      <td>[[that, was, big], [yes], [I'm, doing, fine], ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>[[all, right], [okay], [oh, actually, before, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447</td>\n",
       "      <td>[[yeah, that's, perfectly, fine], [I'm, feelin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression\n",
       "0   369  [[okay, awesome, thank, you], [are, you, okay,...           0\n",
       "1   423  [[okay], [and, please], [yes], [feeling, well]...           0\n",
       "2   436  [[that, was, big], [yes], [I'm, doing, fine], ...           0\n",
       "3   318  [[all, right], [okay], [oh, actually, before, ...           0\n",
       "4   447  [[yeah, that's, perfectly, fine], [I'm, feelin...           0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a455f9dc-d71b-4a93-b984-60910a7e740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_data(text_data):\n",
    "    # For each sublist, join the words in lowercase with spaces and add a period\n",
    "    return [' '.join(word.lower() for word in sublist) + '.' for sublist in text_data]\n",
    "\n",
    "# Apply the function to each entry in the 'Text Data' column\n",
    "df['Processed Text Data'] = df['Text Data'].apply(process_text_data)\n",
    "df['Processed Text Data'] = df['Processed Text Data'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22916eaf-7124-4b17-a0c2-83ee2ab9fdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "      <th>Processed Text Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369</td>\n",
       "      <td>[[okay, awesome, thank, you], [are, you, okay,...</td>\n",
       "      <td>0</td>\n",
       "      <td>okay awesome thank you. are you okay with this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423</td>\n",
       "      <td>[[okay], [and, please], [yes], [feeling, well]...</td>\n",
       "      <td>0</td>\n",
       "      <td>okay. and please. yes. feeling well. where are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>436</td>\n",
       "      <td>[[that, was, big], [yes], [I'm, doing, fine], ...</td>\n",
       "      <td>0</td>\n",
       "      <td>that was big. yes. i'm doing fine. mexico. whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>[[all, right], [okay], [oh, actually, before, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>all right. okay. oh actually before that one t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447</td>\n",
       "      <td>[[yeah, that's, perfectly, fine], [I'm, feelin...</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah that's perfectly fine. i'm feeling great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression  \\\n",
       "0   369  [[okay, awesome, thank, you], [are, you, okay,...           0   \n",
       "1   423  [[okay], [and, please], [yes], [feeling, well]...           0   \n",
       "2   436  [[that, was, big], [yes], [I'm, doing, fine], ...           0   \n",
       "3   318  [[all, right], [okay], [oh, actually, before, ...           0   \n",
       "4   447  [[yeah, that's, perfectly, fine], [I'm, feelin...           0   \n",
       "\n",
       "                                 Processed Text Data  \n",
       "0  okay awesome thank you. are you okay with this...  \n",
       "1  okay. and please. yes. feeling well. where are...  \n",
       "2  that was big. yes. i'm doing fine. mexico. whe...  \n",
       "3  all right. okay. oh actually before that one t...  \n",
       "4  yeah that's perfectly fine. i'm feeling great ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88036698-dc06-4841-8620-d0053affb443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be57f73c-346b-484a-8e86-feafc3af56e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is opt: <tf_keras.src.optimizers.adam.Adam object at 0x7f522b6a5e70>\n"
     ]
    }
   ],
   "source": [
    "  opt = tf.keras.optimizers.Adam(1e-5)\n",
    "  print(f\"this is opt: {opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc9da207-e74c-436a-bf70-ca030d22fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with seed: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is opt: <tf_keras.src.optimizers.adam.Adam object at 0x7f50fd434bb0>\n",
      "Fold 1/5\n",
      "Epoch 1/50\n",
      " 2/18 [==>...........................] - ETA: 2:17 - loss: 0.7149 - accuracy: 0.5625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     72\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation data for this fold\u001b[39;00m\n\u001b[1;32m     75\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(valid_dataset\u001b[38;5;241m.\u001b[39mbatch(batch_size))\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tf_keras/src/engine/training.py:1804\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1798\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1802\u001b[0m ):\n\u001b[1;32m   1803\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1804\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1806\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m/data0/home/h24/chso7162/myenv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "seed_values = list(range(6,7))\n",
    "\n",
    "for seed in seed_values:\n",
    "    print(f\"Running with seed: {seed}\")\n",
    "    data = df\n",
    "    # Set the seed for Python random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the seed for TensorFlow\n",
    "    tf.random.set_seed(seed)\n",
    "    # Initialize the stratified k-fold cross-validator\n",
    "    kf = StratifiedKFold(n_splits=k,random_state=seed, shuffle=True) # ensure the shuffling is the reproducible\n",
    "    # Load the pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    # Define a loss function and metrics\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")]\n",
    "    # Define an optimizer\n",
    "    opt = tf.keras.optimizers.Adam(1e-5)\n",
    "    print(f\"this is opt: {opt}\")\n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    # tf.keras.optimizers.Optimizer(\n",
    "    # name='adam',\n",
    "    # learning_rate=0.001,\n",
    "    # weight_decay=0,\n",
    "    # clipnorm=None,\n",
    "    # clipvalue=None,\n",
    "    # global_clipnorm=None,\n",
    "    # use_ema=False,\n",
    "    # ema_momentum=0.99,\n",
    "    # ema_overwrite_frequency=None,\n",
    "    # jit_compile=True,\n",
    "    # )\n",
    "\n",
    "    # Split the dataset into features and target arrays\n",
    "    texts = df['Processed Text Data'].values\n",
    "    labels = data['depression'].values\n",
    "    fold_results = []\n",
    "    # i=1\n",
    "    # Iterate through the k folds\n",
    "\n",
    "    for fold, (train_val_index, test_index) in enumerate(kf.split(texts, labels)):\n",
    "      # Set the seed for each fold (to ensure the reproducibility of each fold, these are fold-specific seeds)\n",
    "      tf.random.set_seed(seed)\n",
    "      np.random.seed(seed)\n",
    "      random.seed(seed)\n",
    "      # Split train_val set to train and validation sets in 8:2 ratio\n",
    "      train_index, valid_index = train_test_split(train_val_index, test_size=0.2, stratify=labels[train_val_index], random_state=seed) #ensure the splitting is reproducible\n",
    "      print(f\"Fold {fold + 1}/{k}\")\n",
    "      # Create train, validation, and test datasets for this fold\n",
    "      train_texts, train_labels = texts[train_index], labels[train_index]\n",
    "      valid_texts, valid_labels = texts[valid_index], labels[valid_index]\n",
    "      test_texts, test_labels = texts[test_index], labels[test_index]\n",
    "      # Tokenize the input texts for the fold\n",
    "      train_inputs = tokenizer(list(train_texts), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "      valid_inputs = tokenizer(list(valid_texts), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "      test_inputs = tokenizer(list(test_texts), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "      # Define TensorFlow Datasets for training and validation\n",
    "      train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_inputs), train_labels))\n",
    "      valid_dataset = tf.data.Dataset.from_tensor_slices((dict(valid_inputs), valid_labels))\n",
    "      test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_inputs), test_labels))\n",
    "      # Compile the model for this fold\n",
    "      model.compile(optimizer=opt, loss=loss_fn, metrics=metrics)\n",
    "      # Train the model for this fold\n",
    "      batch_size = 8\n",
    "      num_epochs = 50\n",
    "      model.fit(train_dataset.shuffle(100).batch(batch_size), epochs=num_epochs, validation_data=valid_dataset.batch(batch_size))\n",
    "      # Evaluate the model on the validation data for this fold\n",
    "      loss, accuracy = model.evaluate(valid_dataset.batch(batch_size))\n",
    "      print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n",
    "      model_save_path = f\"../sample_data/model/model{fold + 1}\"\n",
    "      model.save_pretrained(model_save_path)\n",
    "      # Create a DataFrame for test data\n",
    "      test_data_df = pd.DataFrame({'Case':test_index,'text': test_texts, 'label': test_labels})\n",
    "      # Save the test data to a CSV file\n",
    "      test_data_csv_path = f\"../sample_data/dataset/test{fold + 1}.csv\"\n",
    "      test_data_df.to_csv(test_data_csv_path, index=False)\n",
    "    sleep(2)\n",
    "\n",
    "    final_results = []\n",
    "    for fold in range(k):\n",
    "        # Load the model for the current fold\n",
    "        model_path = f\"../sample_data/model/model{fold + 1}\"\n",
    "        loaded_model = TFBertForSequenceClassification.from_pretrained(model_path)\n",
    "        # Load the test data for the current fold\n",
    "        data_path = f\"../sample_data/dataset/test{fold + 1}.csv\"\n",
    "        data = pd.read_csv(data_path)\n",
    "\n",
    "        # Prepare to store results\n",
    "        predictions = []\n",
    "        predicted_probabilities_list0 = []\n",
    "        predicted_probabilities_list1 = []\n",
    "        # predicted_probabilities_list2 = []\n",
    "        # predicted_probabilities_list3 = []\n",
    "        # Iterate over each row in the data\n",
    "        for text in data['text']:\n",
    "            test_inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "            prediction = loaded_model(test_inputs)\n",
    "            predicted_probabilities = tf.nn.softmax(prediction.logits, axis=1).numpy()\n",
    "            predicted_class = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "            predictions.append(predicted_class[0])\n",
    "            predicted_probabilities_list0.append(predicted_probabilities[0][0])\n",
    "            predicted_probabilities_list1.append(predicted_probabilities[0][1])\n",
    "            # predicted_probabilities_list2.append(predicted_probabilities[0][2])\n",
    "            # predicted_probabilities_list3.append(predicted_probabilities[0][3])\n",
    "        # Store results in the DataFrame\n",
    "        data['predicted_class'] = predictions\n",
    "        data['Fold_Model'] = fold + 1\n",
    "        data['prob_class_0'] = predicted_probabilities_list0\n",
    "        data['prob_class_1'] = predicted_probabilities_list1\n",
    "        # data['prob_class_2'] = predicted_probabilities_list2\n",
    "        # data['prob_class_3'] = predicted_probabilities_list3\n",
    "\n",
    "        # Append the results of this fold to the final results\n",
    "        final_results.append(data)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    final_result_data = pd.concat(final_results)\n",
    "    sleep(2)\n",
    "    final_result_data.to_csv(f\"../sample_data/dataset/result_depression_epoch50.csv\")\n",
    "    sleep(2)\n",
    "    # datasets = []\n",
    "    # for i in range(1, 15):\n",
    "    file_path = f\"../sample_data/dataset/result_depression_epoch50.csv\"\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    # datasets.append(dataset)\n",
    "    # merged_df = pd.concat(datasets, ignore_index=True)\n",
    "    # merged_df['label'].value_counts()\n",
    "    # final_merged_dfN=merged_df\n",
    "    label_col = 'label'\n",
    "    prob_columns = ['prob_class_0', 'prob_class_1']  # Add more if needed\n",
    "    def calculate_roc_auc(data, label_col, prob_columns, class_label):\n",
    "        # Drop rows with NaN values in label or probability columns\n",
    "        data = data.dropna(subset=[label_col] + prob_columns)\n",
    "        actual = (data[label_col] == class_label).astype(int)\n",
    "        predicted_prob = data[prob_columns].loc[:, f'prob_class_{class_label}']\n",
    "        fpr, tpr, _ = roc_curve(actual, predicted_prob)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        return fpr, tpr, auc_score\n",
    "\n",
    "    # Calculate mean ROC AUC for all classes\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.zeros_like(mean_fpr)\n",
    "    mean_auc_values = []\n",
    "\n",
    "    # Iterate over each class\n",
    "    for class_label in range(len(prob_columns)):\n",
    "        fpr, tpr, auc_score = calculate_roc_auc(dataset, label_col, prob_columns, class_label)\n",
    "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "        mean_auc_values.append(auc_score)\n",
    "        print(f'AUC for Class {class_label}: {auc_score}')\n",
    "\n",
    "    # Calculate mean TPR and AUC\n",
    "    mean_tpr /= len(prob_columns)\n",
    "    mean_auc = np.mean(mean_auc_values)\n",
    "    print(f'Mean AUC across all classes: {mean_auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
