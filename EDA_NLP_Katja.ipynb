{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data- preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing your CSV files\n",
    "# place the link to the text file here\n",
    "# directory = '/Users/martinso/Desktop/LST/HS-KI/Year 2/research trend/text'\n",
    "directory = r'C:\\awilde\\katja\\Dokumente\\Studium\\Schweden\\HealthInformatics_KarolinskaUni\\Lectures\\03_Semester\\TrendsInHI\\SU_project\\text'\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Initialize a list to store each row's text as a list of words\n",
    "        texts = []\n",
    "\n",
    "        # Loop through each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            # Check for NaN and add text to the list after splitting into words\n",
    "            if pd.notna(row['Text']):\n",
    "                # Split the text into words and append the list of words\n",
    "                texts.append(row['Text'].split())\n",
    "\n",
    "        # Append the filename (without extension) and the list of texts to the data list\n",
    "        data.append([os.path.splitext(filename)[0], texts])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df_combined = pd.DataFrame(data, columns=['File Name', 'Text Data'])\n",
    "\n",
    "# Define the path for the output Excel file\n",
    "#output_file_path = os.path.join(directory, 'aggregated_texts_by-sentence.xlsx')\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "#df_combined.to_excel(output_file_path, index=False)\n",
    "\n",
    "\n",
    "#print(\"All files have been processed and output to 'aggregated_texts.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Text Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300_Transcript</td>\n",
       "      <td>[[so, I'm, going, to], [interview, in, Spanish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301_Transcript</td>\n",
       "      <td>[[yeah, there's, also, on, Craigslist, so, tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302_Transcript</td>\n",
       "      <td>[[just, move, around, a, little, bit], [when, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303_Transcript</td>\n",
       "      <td>[[wow, okay], [when, you're, finished, when, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304_Transcript</td>\n",
       "      <td>[[so, we'll, just, move, around, a, little, bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        File Name                                          Text Data\n",
       "0  300_Transcript  [[so, I'm, going, to], [interview, in, Spanish...\n",
       "1  301_Transcript  [[yeah, there's, also, on, Craigslist, so, tha...\n",
       "2  302_Transcript  [[just, move, around, a, little, bit], [when, ...\n",
       "3  303_Transcript  [[wow, okay], [when, you're, finished, when, s...\n",
       "4  304_Transcript  [[so, we'll, just, move, around, a, little, bi..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.rename(columns={'File Name': 'Name'}, inplace=True)\n",
    "df_combined['Name'] = df_combined['Name'].str.replace('_Transcript', '')\n",
    "df_combined['Name']=df_combined['Name'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>[[so, I'm, going, to], [interview, in, Spanish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>[[yeah, there's, also, on, Craigslist, so, tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>[[just, move, around, a, little, bit], [when, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>[[wow, okay], [when, you're, finished, when, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>[[so, we'll, just, move, around, a, little, bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data\n",
       "0   300  [[so, I'm, going, to], [interview, in, Spanish...\n",
       "1   301  [[yeah, there's, also, on, Craigslist, so, tha...\n",
       "2   302  [[just, move, around, a, little, bit], [when, ...\n",
       "3   303  [[wow, okay], [when, you're, finished, when, s...\n",
       "4   304  [[so, we'll, just, move, around, a, little, bi..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('label_daic_extend - label_daic_extend.csv')\n",
    "df.head()\n",
    "df_filtered = df[['Participant_ID','depression']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_map = df_filtered.set_index('Participant_ID')['depression']\n",
    "\n",
    "df_combined['depression'] = df_combined['Name'].map(depression_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>[[so, I'm, going, to], [interview, in, Spanish...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>[[yeah, there's, also, on, Craigslist, so, tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>[[just, move, around, a, little, bit], [when, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>[[wow, okay], [when, you're, finished, when, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>[[so, we'll, just, move, around, a, little, bi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>698</td>\n",
       "      <td>[[going, to, press, that, button, it, just, me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>702</td>\n",
       "      <td>[[hi], [I'm, not, a, therapist], [are, you, ok...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>703</td>\n",
       "      <td>[[and, please], [are, you, okay, with, this, y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>707</td>\n",
       "      <td>[[okay, but, but, okay, but, I, don't, stop, u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>713</td>\n",
       "      <td>[[anime, go, ahead, and, start, a, fire, camco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name                                          Text Data  depression\n",
       "0     300  [[so, I'm, going, to], [interview, in, Spanish...           0\n",
       "1     301  [[yeah, there's, also, on, Craigslist, so, tha...           0\n",
       "2     302  [[just, move, around, a, little, bit], [when, ...           0\n",
       "3     303  [[wow, okay], [when, you're, finished, when, s...           0\n",
       "4     304  [[so, we'll, just, move, around, a, little, bi...           0\n",
       "..    ...                                                ...         ...\n",
       "214   698  [[going, to, press, that, button, it, just, me...           1\n",
       "215   702  [[hi], [I'm, not, a, therapist], [are, you, ok...           0\n",
       "216   703  [[and, please], [are, you, okay, with, this, y...           0\n",
       "217   707  [[okay, but, but, okay, but, I, don't, stop, u...           0\n",
       "218   713  [[anime, go, ahead, and, start, a, fire, camco...           0\n",
       "\n",
       "[219 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of interviewee with depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of patients without depression: 70.32%\n",
      "Percentage of patients with depression: 29.68%\n"
     ]
    }
   ],
   "source": [
    "# Calculate value counts for depression labels\n",
    "value_counts = df_combined['depression'].value_counts()\n",
    "\n",
    "# Calculate percentage for each label\n",
    "percentages = (value_counts / value_counts.sum()) * 100\n",
    "print(f\"Percentage of patients without depression: {percentages[0]:.2f}%\")\n",
    "print(f\"Percentage of patients with depression: {percentages[1]:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem like there is a imbalanced class in the data, we should take account of that when we try to build up the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['lengths of speech'] = df_combined['Text Data'].apply(lambda x: [len(lst) for lst in x])\n",
    "# Count the number of inner lists in each cell and create a new column\n",
    "df_combined['frequency of speech'] = df_combined['Text Data'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "      <th>lengths of speech</th>\n",
       "      <th>frequency of speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>[[so, I'm, going, to], [interview, in, Spanish...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, 3, 1, 1, 2, 5, 3, 8, 3, 2, 1, 2, 6, 22, 7,...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>[[yeah, there's, also, on, Craigslist, so, tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8, 1, 10, 4, 2, 9, 3, 4, 13, 5, 15, 1, 19, 22...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>[[just, move, around, a, little, bit], [when, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 3, 5, 5, 3, 4, 2, 1, 5, 10, 3, 13, 8, 25, ...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>[[wow, okay], [when, you're, finished, when, s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 6, 1, 2, 9, 3, 14, 19, 2, 43, 4, 11, 6, 2,...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>[[so, we'll, just, move, around, a, little, bi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[26, 10, 2, 8, 3, 4, 9, 2, 1, 4, 5, 8, 2, 18, ...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression  \\\n",
       "0   300  [[so, I'm, going, to], [interview, in, Spanish...           0   \n",
       "1   301  [[yeah, there's, also, on, Craigslist, so, tha...           0   \n",
       "2   302  [[just, move, around, a, little, bit], [when, ...           0   \n",
       "3   303  [[wow, okay], [when, you're, finished, when, s...           0   \n",
       "4   304  [[so, we'll, just, move, around, a, little, bi...           0   \n",
       "\n",
       "                                   lengths of speech  frequency of speech  \n",
       "0  [4, 3, 1, 1, 2, 5, 3, 8, 3, 2, 1, 2, 6, 22, 7,...                   77  \n",
       "1  [8, 1, 10, 4, 2, 9, 3, 4, 13, 5, 15, 1, 19, 22...                   72  \n",
       "2  [6, 3, 5, 5, 3, 4, 2, 1, 5, 10, 3, 13, 8, 25, ...                   99  \n",
       "3  [2, 6, 1, 2, 9, 3, 14, 19, 2, 43, 4, 11, 6, 2,...                   94  \n",
       "4  [26, 10, 2, 8, 3, 4, 9, 2, 1, 4, 5, 8, 2, 18, ...                   80  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i try to calculate the frequency of interviewees speak in the interview and the length of their speech per times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(lengths):\n",
    "    if len(lengths) > 0:  # Check if the list is not empty to avoid division by zero\n",
    "        return sum(lengths) / len(lengths)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "df_combined['mean length of speech'] = df_combined['lengths of speech'].apply(calculate_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            frequency of speech  mean length of speech\n",
      "depression                                            \n",
      "0                     88.642857              15.954897\n",
      "1                    105.553846              14.008058\n"
     ]
    }
   ],
   "source": [
    "mean_values = df_combined.groupby('depression')[['frequency of speech', 'mean length of speech']].mean()\n",
    "\n",
    "print(mean_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant difference in frequency of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_text(row):\n",
    "    # This will flatten the list of lists found in 'Text Data' for a row\n",
    "    flattened = [item for sublist in row['Text Data'] for item in sublist]\n",
    "    return flattened  # Returning the flattened list directly\n",
    "\n",
    "# Apply this function and create a new column\n",
    "df_combined['flattened text'] = df_combined.apply(flatten_text, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "      <th>lengths of speech</th>\n",
       "      <th>frequency of speech</th>\n",
       "      <th>mean length of speech</th>\n",
       "      <th>flattened text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>[[so, I'm, going, to], [interview, in, Spanish...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, 3, 1, 1, 2, 5, 3, 8, 3, 2, 1, 2, 6, 22, 7,...</td>\n",
       "      <td>77</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>[so, I'm, going, to, interview, in, Spanish, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>[[yeah, there's, also, on, Craigslist, so, tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8, 1, 10, 4, 2, 9, 3, 4, 13, 5, 15, 1, 19, 22...</td>\n",
       "      <td>72</td>\n",
       "      <td>19.430556</td>\n",
       "      <td>[yeah, there's, also, on, Craigslist, so, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>[[just, move, around, a, little, bit], [when, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 3, 5, 5, 3, 4, 2, 1, 5, 10, 3, 13, 8, 25, ...</td>\n",
       "      <td>99</td>\n",
       "      <td>6.151515</td>\n",
       "      <td>[just, move, around, a, little, bit, when, you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>[[wow, okay], [when, you're, finished, when, s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 6, 1, 2, 9, 3, 14, 19, 2, 43, 4, 11, 6, 2,...</td>\n",
       "      <td>94</td>\n",
       "      <td>20.382979</td>\n",
       "      <td>[wow, okay, when, you're, finished, when, she'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>[[so, we'll, just, move, around, a, little, bi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[26, 10, 2, 8, 3, 4, 9, 2, 1, 4, 5, 8, 2, 18, ...</td>\n",
       "      <td>80</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>[so, we'll, just, move, around, a, little, bit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression  \\\n",
       "0   300  [[so, I'm, going, to], [interview, in, Spanish...           0   \n",
       "1   301  [[yeah, there's, also, on, Craigslist, so, tha...           0   \n",
       "2   302  [[just, move, around, a, little, bit], [when, ...           0   \n",
       "3   303  [[wow, okay], [when, you're, finished, when, s...           0   \n",
       "4   304  [[so, we'll, just, move, around, a, little, bi...           0   \n",
       "\n",
       "                                   lengths of speech  frequency of speech  \\\n",
       "0  [4, 3, 1, 1, 2, 5, 3, 8, 3, 2, 1, 2, 6, 22, 7,...                   77   \n",
       "1  [8, 1, 10, 4, 2, 9, 3, 4, 13, 5, 15, 1, 19, 22...                   72   \n",
       "2  [6, 3, 5, 5, 3, 4, 2, 1, 5, 10, 3, 13, 8, 25, ...                   99   \n",
       "3  [2, 6, 1, 2, 9, 3, 14, 19, 2, 43, 4, 11, 6, 2,...                   94   \n",
       "4  [26, 10, 2, 8, 3, 4, 9, 2, 1, 4, 5, 8, 2, 18, ...                   80   \n",
       "\n",
       "   mean length of speech                                     flattened text  \n",
       "0               4.181818  [so, I'm, going, to, interview, in, Spanish, o...  \n",
       "1              19.430556  [yeah, there's, also, on, Craigslist, so, that...  \n",
       "2               6.151515  [just, move, around, a, little, bit, when, you...  \n",
       "3              20.382979  [wow, okay, when, you're, finished, when, she'...  \n",
       "4              12.400000  [so, we'll, just, move, around, a, little, bit...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word_length(words):\n",
    "    if not words:  # Check if the list is empty\n",
    "        return 0\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# Apply the function to calculate the mean word length for each row\n",
    "df_combined['mean_word_length'] = df_combined['flattened text'].apply(mean_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Data</th>\n",
       "      <th>depression</th>\n",
       "      <th>lengths of speech</th>\n",
       "      <th>frequency of speech</th>\n",
       "      <th>mean length of speech</th>\n",
       "      <th>flattened text</th>\n",
       "      <th>mean_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>[[so, I'm, going, to], [interview, in, Spanish...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, 3, 1, 1, 2, 5, 3, 8, 3, 2, 1, 2, 6, 22, 7,...</td>\n",
       "      <td>77</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>[so, I'm, going, to, interview, in, Spanish, o...</td>\n",
       "      <td>4.068323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>[[yeah, there's, also, on, Craigslist, so, tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8, 1, 10, 4, 2, 9, 3, 4, 13, 5, 15, 1, 19, 22...</td>\n",
       "      <td>72</td>\n",
       "      <td>19.430556</td>\n",
       "      <td>[yeah, there's, also, on, Craigslist, so, that...</td>\n",
       "      <td>3.961401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>[[just, move, around, a, little, bit], [when, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 3, 5, 5, 3, 4, 2, 1, 5, 10, 3, 13, 8, 25, ...</td>\n",
       "      <td>99</td>\n",
       "      <td>6.151515</td>\n",
       "      <td>[just, move, around, a, little, bit, when, you...</td>\n",
       "      <td>4.096880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>[[wow, okay], [when, you're, finished, when, s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 6, 1, 2, 9, 3, 14, 19, 2, 43, 4, 11, 6, 2,...</td>\n",
       "      <td>94</td>\n",
       "      <td>20.382979</td>\n",
       "      <td>[wow, okay, when, you're, finished, when, she'...</td>\n",
       "      <td>3.914927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>[[so, we'll, just, move, around, a, little, bi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[26, 10, 2, 8, 3, 4, 9, 2, 1, 4, 5, 8, 2, 18, ...</td>\n",
       "      <td>80</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>[so, we'll, just, move, around, a, little, bit...</td>\n",
       "      <td>4.021169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name                                          Text Data  depression  \\\n",
       "0   300  [[so, I'm, going, to], [interview, in, Spanish...           0   \n",
       "1   301  [[yeah, there's, also, on, Craigslist, so, tha...           0   \n",
       "2   302  [[just, move, around, a, little, bit], [when, ...           0   \n",
       "3   303  [[wow, okay], [when, you're, finished, when, s...           0   \n",
       "4   304  [[so, we'll, just, move, around, a, little, bi...           0   \n",
       "\n",
       "                                   lengths of speech  frequency of speech  \\\n",
       "0  [4, 3, 1, 1, 2, 5, 3, 8, 3, 2, 1, 2, 6, 22, 7,...                   77   \n",
       "1  [8, 1, 10, 4, 2, 9, 3, 4, 13, 5, 15, 1, 19, 22...                   72   \n",
       "2  [6, 3, 5, 5, 3, 4, 2, 1, 5, 10, 3, 13, 8, 25, ...                   99   \n",
       "3  [2, 6, 1, 2, 9, 3, 14, 19, 2, 43, 4, 11, 6, 2,...                   94   \n",
       "4  [26, 10, 2, 8, 3, 4, 9, 2, 1, 4, 5, 8, 2, 18, ...                   80   \n",
       "\n",
       "   mean length of speech                                     flattened text  \\\n",
       "0               4.181818  [so, I'm, going, to, interview, in, Spanish, o...   \n",
       "1              19.430556  [yeah, there's, also, on, Craigslist, so, that...   \n",
       "2               6.151515  [just, move, around, a, little, bit, when, you...   \n",
       "3              20.382979  [wow, okay, when, you're, finished, when, she'...   \n",
       "4              12.400000  [so, we'll, just, move, around, a, little, bit...   \n",
       "\n",
       "   mean_word_length  \n",
       "0          4.068323  \n",
       "1          3.961401  \n",
       "2          4.096880  \n",
       "3          3.914927  \n",
       "4          4.021169  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_combined is already defined and includes the necessary columns\n",
    "# Convert 'depression' to a categorical type if it's not already\n",
    "df_combined['depression'] = df_combined['depression'].astype('category')\n",
    "\n",
    "def visualize(col):\n",
    "    # Set up the figure and axes\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # figsize can be adjusted based on your display preferences\n",
    "\n",
    "    # First plot: Boxplot\n",
    "    sns.boxplot(ax=axes[0], y=df_combined[col], x=df_combined['depression'],hue=df_combined['depression'])\n",
    "    axes[0].set_ylabel(col, labelpad=12.5)\n",
    "    axes[0].set_xlabel('Depression')\n",
    "    \n",
    "    # Second plot: KDE Plot\n",
    "    sns.kdeplot(ax=axes[1], x=df_combined[col], hue=df_combined['depression'], common_norm=False)\n",
    "    plt.legend(title='Depression', labels=df_combined['depression'].cat.categories)\n",
    "    axes[1].set_xlabel('')\n",
    "    axes[1].set_ylabel('')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# visualize('mean_word_length')  # replace 'mean_word_length' with the actual column name you want to plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['frequency of speech','mean length of speech','mean_word_length']\n",
    "for feature in features:\n",
    "    visualize(feature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further analysis of the speech of interviewees, it seem like there is a higher chance for interviewee with depression to have a shorter speech per times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following I will try to normalise the text, and show the importance of normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_apostrophe_words(text_list):\n",
    "    # Regex pattern to find words with apostrophes\n",
    "    pattern = r\"\\b\\w+'(?:\\w+|t)\\b|\\B'\\w+\\b\"\n",
    "    apo_terms = []  # Initialize an empty list to collect all terms\n",
    "    for item in text_list:\n",
    "        # Find all matches and extend them to the apo_terms list\n",
    "        apo_terms.extend(re.findall(pattern, item))\n",
    "    return apo_terms\n",
    "\n",
    "# Assuming df_combined['flattened text'] contains lists of strings\n",
    "all_words_with_apostrophes = [word for text in df_combined['flattened text'].dropna() for word in find_apostrophe_words(text)]\n",
    "\n",
    "# Convert list to a set to remove duplicates, then back to list to print or use further\n",
    "unique_words_with_apostrophes = list(set(all_words_with_apostrophes))\n",
    "\n",
    "# Print unique words\n",
    "print(unique_words_with_apostrophes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def tokenize_and_flatten(text_list):\n",
    "    tokenized_list = [tokenizer.tokenize(word) for word in text_list]\n",
    "    # Flatten the list of lists into a single list\n",
    "    flat_list = [item for sublist in tokenized_list for item in sublist]\n",
    "    return flat_list\n",
    "\n",
    "# Apply the tokenizer to the 'flattened text' column and store the results in a new column\n",
    "df_combined['tokenized_text'] = df_combined['flattened text'].apply(tokenize_and_flatten)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_with_apostrophes = [word for text in df_combined['tokenized_text'].dropna() for word in find_apostrophe_words(text)]\n",
    "\n",
    "# Convert list to a set to remove duplicates, then back to list to print or use further\n",
    "unique_words_with_apostrophes = list(set(all_words_with_apostrophes))\n",
    "\n",
    "# Print unique words\n",
    "print(unique_words_with_apostrophes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "stop=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"'s\": \"is\",     # depending on context, can also be 'has' or possessive marker\n",
    "    \"n't\": \"not\",\n",
    "    \"'m\": \"am\",\n",
    "    \"'d\": \"would\",  # can also mean \"had\" depending on context\n",
    "    \"'ve\": \"have\",\n",
    "    \"s'more\": \"some more\" ,\n",
    "    \"Hold'em\": \"hold them\",\n",
    "    \"y'alls\": \"you alls\",\n",
    "    \"I'ma\": \"I am going to\",\n",
    "    \"'ll\": \"will\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"'re\": \"are\",\n",
    "    \"y'all\": \"you all\"\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assume 'df_combined' is your DataFrame and contractions_dict is defined as provided\n",
    "\n",
    "def expand_contractions(tokens, contractions_dict):\n",
    "    expanded_tokens = []\n",
    "    for token in tokens:\n",
    "        # Check each token if it's a contraction\n",
    "        if token.lower() in contractions_dict:\n",
    "            # If it is, replace it with the expanded form\n",
    "            expanded_tokens.extend(contractions_dict[token.lower()].split())\n",
    "        else:\n",
    "            # Otherwise, keep the token as is\n",
    "            expanded_tokens.append(token)\n",
    "    return expanded_tokens\n",
    "\n",
    "# Apply this function to each row in the tokenized_text column\n",
    "df_combined['expanded_text'] = df_combined['tokenized_text'].apply(lambda x: expand_contractions(x, contractions_dict))\n",
    "df_combined['expanded_text']  =df_combined['expanded_text'].apply(lambda x: ' '.join(x))\n",
    "df_combined['expanded_text'] = df_combined['expanded_text'].apply(lambda x: x.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to an Excel file\n",
    "file_path = 'depression_detection_preprocessed.xlsx'  # specify the name of your file\n",
    "df_combined.to_excel(file_path, index=False)  # index=False to avoid saving the DataFrame index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set up stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming 'expanded_text' is already a list of words per document\n",
    "# If 'expanded_text' is not a list of words, uncomment the following line:\n",
    "# df_combined['expanded_text'] = df_combined['expanded_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Flatten the list of lists into a single list of words\n",
    "corpus = [word for sublist in df_combined['expanded_text'] for word in sublist]\n",
    "\n",
    "# Dictionary to count occurrences of each stopword\n",
    "dic = defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word] += 1\n",
    "\n",
    "# Sorting the dictionary by frequency of stopwords and getting the top 10\n",
    "top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "x, y = zip(*top)  # Unpack the top words and their counts\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x, y)\n",
    "plt.title('Top Stopwords in Expanded Text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_non_stopwords_barchart(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # Flatten the list of lists into a single list if necessary\n",
    "    # Assuming 'text' is a list of lists of words; if not, comment out the next two lines\n",
    "    text = [word for sublist in text for word in sublist]\n",
    "    \n",
    "    counter = Counter(text)\n",
    "    most = counter.most_common()\n",
    "    x, y = [], []\n",
    "    for word, count in most[:40]:\n",
    "        if word.lower() not in stop:  # Check against stopwords using lowercased words\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=y, y=x)\n",
    "    plt.title('Top Non-Stopwords in tokenized Text')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Words')\n",
    "    plt.show()\n",
    "\n",
    "# Apply the function to the 'expanded_text' column\n",
    "plot_top_non_stopwords_barchart(df_combined['tokenized_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_non_stopwords_barchart(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # Flatten the list of lists into a single list if necessary\n",
    "    # Assuming 'text' is a list of lists of words; if not, comment out the next two lines\n",
    "    text = [word for sublist in text for word in sublist]\n",
    "    \n",
    "    counter = Counter(text)\n",
    "    most = counter.most_common()\n",
    "    x, y = [], []\n",
    "    for word, count in most[:40]:\n",
    "        if word.lower() not in stop:  # Check against stopwords using lowercased words\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=y, y=x)\n",
    "    plt.title('Top Non-Stopwords in expanded text')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Words')\n",
    "    plt.show()\n",
    "\n",
    "plot_top_non_stopwords_barchart(df_combined['expanded_text'])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. rule-based \n",
    "https://neptune.ai/blog/sentiment-analysis-python-textblob-vs-vader-vs-flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Function to apply VADER and return the compound score\n",
    "def apply_vader(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "# Assuming 'expanded_text' column is a list of words, join them into a single string per row\n",
    "df_combined['expanded_text_str'] = df_combined['expanded_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Apply the VADER analysis on the joined text\n",
    "df_combined['vader_sentiment'] = df_combined['expanded_text_str'].apply(apply_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vader_sentiment = df_combined.groupby('depression')[['vader_sentiment']].mean()\n",
    "\n",
    "print(mean_vader_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both groups show relatively high average sentiment scores, which implies that, on average, the transcripts have positive sentiment regardless of depression label. However, the depression group (1) has a slightly lower mean score than the non-depression group (0), which might suggest a subtle trend towards less positive sentiment in the speech of individuals with depression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Common Words and Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Separate text by depression label\n",
    "depressed_text = ' '.join(df_combined[df_combined['depression'] == 1]['expanded_text_str'].tolist())\n",
    "non_depressed_text = ' '.join(df_combined[df_combined['depression'] == 0]['expanded_text_str'].tolist())\n",
    "\n",
    "# Generate word clouds\n",
    "wordcloud_depressed = WordCloud(width=800, height=400, max_words=100).generate(depressed_text)\n",
    "wordcloud_non_depressed = WordCloud(width=800, height=400, max_words=100).generate(non_depressed_text)\n",
    "\n",
    "# Plot the word clouds side-by-side\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_depressed, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Depressed Group')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_non_depressed, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Non-Depressed Group')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. embedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair # A natural language processing (NLP) library\n",
    "# specific classes from the Flair library used for building and processing sentences\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Load the sentiment classifier\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "# function takes a text input, converts it into a Sentence object, and predicts its sentiment using the loaded classifier\n",
    "# It returns the sentiment label (e.g., \"POSITIVE\" or \"NEGATIVE\") and the associated confidence score (a float between 0 and 1) for that prediction\n",
    "def flair_sentiment(text):\n",
    "    # Make sure text is a string\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    # Flair outputs labels with additional information, extracting sentiment and score\n",
    "    return sentence.labels[0].value, sentence.labels[0].score\n",
    "\n",
    "df_combined['flair_sentiment'], df_combined['flair_score'] = zip(*df_combined['expanded_text_str'].apply(flair_sentiment))\n",
    "\n",
    "# View the DataFrame to see the added sentiment analysis results\n",
    "print(df_combined[['expanded_text_str', 'flair_sentiment', 'flair_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "- Responses labeled as POSITIVE indicate that the text has an overall optimistic or favorable tone. These individuals may express feelings of hope, happiness, or contentment.\n",
    "- Responses labeled as NEGATIVE reflect a pessimistic or unfavorable tone. This might include expressions of sadness, frustration, or hopelessness.\n",
    "- flair_score: This column provides a confidence score for the sentiment prediction, indicating how certain the model is about its classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where 'depression' is 1\n",
    "depression_df = df_combined[df_combined['depression'] == 1]\n",
    "\n",
    "# Count the occurrences of each sentiment within the depression class\n",
    "sentiment_counts = depression_df['flair_sentiment'].value_counts()\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sentiment_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Sentiment Distribution in Depression Class')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each sentiment\n",
    "sentiment_counts = df_combined['flair_sentiment'].value_counts()\n",
    "total_count = len(df_combined)\n",
    "\n",
    "# Calculate percentages\n",
    "sentiment_percentage = (sentiment_counts / total_count) * 100\n",
    "\n",
    "# Display results\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(sentiment_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing sentiment by depression label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with sentiment scores and depression labels\n",
    "sentiment_by_depression = df_combined[['depression', 'flair_score']]\n",
    "\n",
    "# Calculate mean sentiment scores by depression label\n",
    "mean_sentiment = sentiment_by_depression.groupby('depression')['flair_score'].mean()\n",
    "\n",
    "# Display mean sentiment scores\n",
    "print(\"Mean Sentiment Scores by Depression Label:\")\n",
    "print(mean_sentiment)\n",
    "\n",
    "# Visualize the results\n",
    "sns.barplot(x=mean_sentiment.index, y=mean_sentiment.values)\n",
    "plt.xticks([0, 1], ['Not Depressed (0)', 'Depressed (1)'])\n",
    "plt.ylabel('Mean Sentiment Score')\n",
    "plt.title('Mean Sentiment Scores by Depression Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both groups (depressed and not depressed) exhibit high mean sentiment scores, indicating that the language used in their responses leans towards positivity. This could suggest that even individuals experiencing depression may still express positive sentiments in certain contexts or discussions.\n",
    "\n",
    "The mean sentiment score for the depressed group (0.892106) is higher than that of the non-depressed group (0.871376). This might seem counterintuitive because one would typically expect those who are depressed to express more negative sentiments. However, it could indicate that the topics discussed or the context of the interviews elicited positive responses even from those with depression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion detection\n",
    "- Aim: to identify and categorize the specific emotions expressed in text\n",
    "- can classify text into multiple emotional categories (e.g. happiness, sadness, anger, fear, surprise, and disgust)\n",
    "- eight primary emotions found in a paper: joy, trust, fear, surprise, sadness, aversion, anger, and anticipation\n",
    "\n",
    "Steps for emotion detection\n",
    "- choose a pre-trained model (e.g. model in the Huggung Face transformers library)\n",
    "- prepare text data (if not already done) -> cleaning, tokenizing, converting in appropriate format\n",
    "- apply the model\n",
    "- analyze the results\n",
    "    - the output typically includes a predicted emotion (dominant emotion detected in the text) and a confidence score (between 0 and 1, indicating the model's confidence in its prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Load an emotion detection model\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"nateraw/bert-base-uncased-emotion\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\")\n",
    "\n",
    "def split_text(text, max_length=512):\n",
    "    # Tokenize the text to get the token count\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    segments = []\n",
    "\n",
    "    # Split the tokens into segments based on the maximum length\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        segment_tokens = tokens[i:i + max_length]\n",
    "        # Decode the token segment back to string\n",
    "        segments.append(tokenizer.decode(segment_tokens, skip_special_tokens=True))\n",
    "\n",
    "    return segments\n",
    "\n",
    "def detect_emotion(text):\n",
    "    segments = split_text(text)\n",
    "    emotions = []\n",
    "    \n",
    "    # Collect results from all segments\n",
    "    for segment in segments:\n",
    "        # Ensure that we only classify segments within the limit\n",
    "        if len(tokenizer.encode(segment)) <= 512 and segment.strip():\n",
    "            emotions.extend(emotion_classifier(segment))\n",
    "    return emotions\n",
    "\n",
    "# Apply emotion detection to your data\n",
    "try:\n",
    "    df_combined['emotion'] = df_combined['expanded_text_str'].apply(detect_emotion)\n",
    "except RuntimeError as e:\n",
    "    print(f\"An error occurred during emotion detection: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Check if 'emotion' column was created before trying to print it\n",
    "if 'emotion' in df_combined.columns:\n",
    "    print(df_combined[['expanded_text_str', 'emotion']])\n",
    "else:\n",
    "    print(\"The 'emotion' column was not created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotion frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the most confident emotion from each entry\n",
    "# Flattening the emotions into a list of (label, score) pairs\n",
    "emotion_labels = []\n",
    "for entry in df_combined['emotion']:\n",
    "    if entry:  # Check if the emotion entry is not empty\n",
    "        # Get the most confident emotion label (the first one in the list)\n",
    "        label = entry[0]['label']\n",
    "        emotion_labels.append(label)\n",
    "\n",
    "# Create a DataFrame from the labels\n",
    "emotion_df = pd.DataFrame(emotion_labels, columns=['emotion'])\n",
    "\n",
    "# Count the frequency of each emotion\n",
    "emotion_counts = emotion_df['emotion'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(emotion_counts)\n",
    "\n",
    "# Plot the emotion counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "emotion_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Emotion Distribution in Texts')\n",
    "plt.xlabel('Emotions')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to make room for labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling\n",
    "- technique used to discover abstract topics within a collection of documents\n",
    "- helps in understanding what themes or subjects are present in the text data without requiring labels for the data\n",
    "\n",
    "Steps\n",
    "- Preprocess the text: Clean the text data by tokenizing, removing stop words, and stemming/lemmatization\n",
    "- Create a dictionary and corpus: A dictionary maps each word to a unique ID, while a corpus represents the texts as bag-of-words (frequency of each word in each document)\n",
    "- Apply a topic modeling algorithm: Use an algorithm like Latent Dirichlet Allocation (LDA) to find topics in the corpus. LDA assumes that each document is a mixture of topics and each topic is a mixture of words.\n",
    "- Analyze the topics: Review the generated topics, which typically consist of the most frequent words associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Ensure you have NLTK stopwords downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example pre-processing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stop words, and lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in text.lower().split() if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess your texts\n",
    "df_combined['tokens'] = df_combined['expanded_text_str'].apply(preprocess_text)\n",
    "\n",
    "# Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(df_combined['tokens'])\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df_combined['tokens']]\n",
    "\n",
    "# Apply LDA\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symptoms detection\n",
    "similar to emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symptoms\n",
    "symptoms = [\n",
    "    \"sadness\", \"pessimism\", \"past failure\", \"loss of pleasure\", \"guilty feeling\", \n",
    "    \"punishment feeling\", \"self-dislike\", \"self-criticalness\", \"suicidal thoughts\", \n",
    "    \"crying\", \"agitation\", \"loss of interest\", \"indecisiveness\", \"worthlessness\", \n",
    "    \"loss of energy\", \"changes in sleeping pattern\", \"irritability\", \"changes in appetite\", \n",
    "    \"concentration difficulty\", \"tiredness\"\n",
    "]\n",
    "\n",
    "# Function to detect symptoms\n",
    "def detect_symptoms(text):\n",
    "    detected_symptoms = []\n",
    "    for symptom in symptoms:\n",
    "        if symptom in text.lower():  # Convert to lowercase for case insensitive matching\n",
    "            detected_symptoms.append(symptom)\n",
    "    return detected_symptoms\n",
    "\n",
    "# Apply symptom detection to your data\n",
    "df_combined['detected_symptoms'] = df_combined['expanded_text_str'].apply(detect_symptoms)\n",
    "\n",
    "# View the results\n",
    "print(df_combined[['expanded_text_str', 'detected_symptoms']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "# Count occurrences of each symptom\n",
    "symptom_counts = {}\n",
    "\n",
    "# Aggregate counts\n",
    "for symptoms_list in df_combined['detected_symptoms']:\n",
    "    for symptom in symptoms_list:\n",
    "        if symptom in symptom_counts:\n",
    "            symptom_counts[symptom] += 1\n",
    "        else:\n",
    "            symptom_counts[symptom] = 1\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "symptom_counts_df = pd.DataFrame(list(symptom_counts.items()), columns=['Symptom', 'Count'])\n",
    "\n",
    "# Display the counts\n",
    "print(symptom_counts_df)\n",
    "\n",
    "# Plot the symptom counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "symptom_counts_df.sort_values('Count', ascending=False, inplace=True)  # Sort by count\n",
    "plt.barh(symptom_counts_df['Symptom'], symptom_counts_df['Count'], color='salmon')\n",
    "plt.title('Frequency of Depression Symptoms in Texts')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Symptoms')\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Noun, Verbs, Adjectives, Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# Ensure you have the required resources\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Calculate Average Frequency\n",
    "def calculate_average_frequency(df, tokens_column):\n",
    "    # Initialize counters\n",
    "    total_nouns = 0\n",
    "    total_verbs = 0\n",
    "    total_adjectives = 0\n",
    "    total_adverbs = 0\n",
    "    \n",
    "    # Iterate over each list of tokens\n",
    "    for tokens in df[tokens_column]:\n",
    "        # Perform POS tagging on the tokens\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        # Count frequencies\n",
    "        nouns = sum(1 for _, pos in pos_tags if pos.startswith('NN'))\n",
    "        verbs = sum(1 for _, pos in pos_tags if pos.startswith('VB'))\n",
    "        adjectives = sum(1 for _, pos in pos_tags if pos.startswith('JJ'))\n",
    "        adverbs = sum(1 for _, pos in pos_tags if pos.startswith('RB'))\n",
    "        \n",
    "        total_nouns += nouns\n",
    "        total_verbs += verbs\n",
    "        total_adjectives += adjectives\n",
    "        total_adverbs += adverbs\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_responses = len(df)\n",
    "    average_nouns = total_nouns / num_responses\n",
    "    average_verbs = total_verbs / num_responses\n",
    "    average_adjectives = total_adjectives / num_responses\n",
    "    average_adverbs = total_adverbs / num_responses\n",
    "    \n",
    "    return {\n",
    "        'Average Nouns': average_nouns,\n",
    "        'Average Verbs': average_verbs,\n",
    "        'Average Adjectives': average_adjectives,\n",
    "        'Average Adverbs': average_adverbs\n",
    "    }\n",
    "\n",
    "# Calculate the average frequencies\n",
    "average_frequencies = calculate_average_frequency(df_combined, 'expanded_text')\n",
    "average_df = pd.DataFrame(list(average_frequencies.items()), columns=['Part of Speech', 'Average Frequency'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(average_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_row_average_frequencies(df, tokens_column):\n",
    "    # Lists to hold average frequencies for each row\n",
    "    avg_nouns = []\n",
    "    avg_verbs = []\n",
    "    avg_adjectives = []\n",
    "    avg_adverbs = []\n",
    "    \n",
    "    # Iterate over each list of tokens\n",
    "    for tokens in df[tokens_column]:\n",
    "        # Perform POS tagging on the tokens\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        # Total counts for this row\n",
    "        nouns_count = sum(1 for _, pos in pos_tags if pos.startswith('NN'))  # Noun tags: NN, NNS, NNP, NNPS\n",
    "        verbs_count = sum(1 for _, pos in pos_tags if pos.startswith('VB'))   # Verb tags: VB, VBD, VBG, VBN, VBP, VBZ\n",
    "        adjectives_count = sum(1 for _, pos in pos_tags if pos.startswith('JJ'))  # Adjective tags: JJ, JJR, JJS\n",
    "        adverbs_count = sum(1 for _, pos in pos_tags if pos.startswith('RB'))  # Adverb tags: RB, RBR, RBS\n",
    "        \n",
    "        # Total number of tokens\n",
    "        total_tokens = len(tokens) if len(tokens) > 0 else 1  # Avoid division by zero\n",
    "        \n",
    "        # Calculate averages for this row\n",
    "        avg_nouns.append(nouns_count / total_tokens)\n",
    "        avg_verbs.append(verbs_count / total_tokens)\n",
    "        avg_adjectives.append(adjectives_count / total_tokens)\n",
    "        avg_adverbs.append(adverbs_count / total_tokens)\n",
    "    \n",
    "    # Add the averages to the DataFrame\n",
    "    df['Average Nouns'] = avg_nouns\n",
    "    df['Average Verbs'] = avg_verbs\n",
    "    df['Average Adjectives'] = avg_adjectives\n",
    "    df['Average Adverbs'] = avg_adverbs\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate the average frequencies for each row\n",
    "df_with_averages = calculate_row_average_frequencies(df_combined, 'expanded_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated DataFrame\n",
    "df_with_averages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of First-Person Related Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_first_person_average_frequency(df, tokens_column):\n",
    "    # List of first-person pronouns\n",
    "    first_person_pronouns = ['i','I', 'me', 'Me', 'my', 'My', 'mine', 'Mine', 'we', 'We', 'us', 'Us', 'our', 'Our', 'ours', 'Ours']\n",
    "    \n",
    "    # List to hold average frequencies for each row\n",
    "    avg_fp_frequencies = []\n",
    "    \n",
    "    # Iterate over each list of tokens\n",
    "    for tokens in df[tokens_column]:\n",
    "        # Count the first-person pronouns in the current row\n",
    "        fp_count = sum(1 for token in tokens if token.lower() in first_person_pronouns)\n",
    "        \n",
    "        # Total number of tokens\n",
    "        total_tokens = len(tokens) if len(tokens) > 0 else 1  # Avoid division by zero\n",
    "        \n",
    "        # Calculate average frequency of first-person pronouns for this row\n",
    "        avg_fp_frequencies.append(fp_count / total_tokens)\n",
    "    \n",
    "    # Add the averages to the DataFrame\n",
    "    df['Average First-Person Frequency'] = avg_fp_frequencies\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate the average first-person frequencies for each row\n",
    "df_with_fp_avg = calculate_first_person_average_frequency(df_combined, 'expanded_text')\n",
    "\n",
    "# Display the updated DataFrame showing expanded text and average first-person frequency\n",
    "print(df_with_fp_avg['Average First-Person Frequency'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
